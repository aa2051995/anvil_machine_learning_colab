{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMG7pu0r5M4b"
   },
   "source": [
    "# **Model Spare Part Price Prediction** \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYtJSPRiaHcj"
   },
   "source": [
    "\\TO-DO 08.06.22:\n",
    "-Grid Search auf 12 Features eingrenzen\n",
    "-Features einzeln benennen --> Features_1\n",
    "-Best Hyperparameter aus der Grid Search übernehmen\n",
    "-Evaluierung des Models: Nested Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTSKeWYtGegE"
   },
   "source": [
    "**Overview:**\n",
    "\n",
    "1. Loading the data\n",
    "2. Partially clean the data, partially the data has already been cleaned in Excel\n",
    "3. Visualise data to identify correlations\n",
    "4. Split into test and training data sets\n",
    "5. Creation of the model\n",
    "6. Training the model\n",
    "7. Testing and validating the model \n",
    "8. Output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EdIc3475gni"
   },
   "source": [
    "### Import relevant libaries\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "cmjNhiETQhA9",
    "outputId": "23784de5-146d-4bdf-dbe8-630de473e875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: anvil-uplink in /usr/local/lib/python3.8/dist-packages (0.4.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from anvil-uplink) (1.15.0)\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: ws4py in /usr/local/lib/python3.8/dist-packages (from anvil-uplink) (0.5.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from anvil-uplink) (0.16.0)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "argparse"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install anvil-uplink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2Mf0t3cMMaVk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict\n",
    "import os\n",
    "import mpl_toolkits\n",
    "import param as pp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import io\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import anvil.server\n",
    "import anvil.media\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import mpl_toolkits\n",
    "import param as pp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ytWP3V7aKFAi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ufNbxnrojcfE"
   },
   "outputs": [],
   "source": [
    "def train_eval(features_col):\n",
    "  train = pd.read_excel(\"/content/train data.xlsx\")\n",
    "  inference_dataset = pd.read_excel(\"/content/test data.xlsx\")\n",
    "  inference_dataset.drop(columns=['ID'],axis=1,inplace=True) \n",
    "  train.drop(columns=['ID'],axis=1,inplace=True) \n",
    "\n",
    "  ep = train['Länge in mm'].value_counts().reset_index()\n",
    "  ep.columns = [\n",
    "      'Länge', \n",
    "      'percent'\n",
    "  ]\n",
    "  ep['percent'] /= len(train)\n",
    "\n",
    "\n",
    "  ep = train['Biegungen'].value_counts().reset_index()\n",
    "  ep.columns = [\n",
    "      'Biegungen', \n",
    "      'percent'\n",
    "  ]\n",
    "  ep['percent'] /= len(train)\n",
    "\n",
    "  labels = {}\n",
    "  for col in train.select_dtypes(exclude = np.number).columns.tolist():\n",
    "      le = LabelEncoder().fit(pd.concat([train[col].astype(str),inference_dataset[col].astype(str)]))   \n",
    "      train[col] = le.transform(train[col].astype(str))\n",
    "      inference_dataset[col] = le.transform(inference_dataset[col].astype(str))\n",
    "      labels [col] = le\n",
    "  print('Categorical columns:', list(labels.keys()))\n",
    "\n",
    "\n",
    "\n",
    "  print(f'Percent of Nans in Train Data : {round(train.isna().sum().sum()/len(train), 2)}')\n",
    "  print(f'Percent of Nans in Test  Data : {round(inference_dataset.isna().sum().sum()/len(inference_dataset), 2)}')\n",
    "\n",
    "  train = train.replace([np.inf, -np.inf], np.nan)\n",
    "  train= train.fillna(train.mean())\n",
    "\n",
    "\n",
    "  inference_dataset = inference_dataset.replace([np.inf, -np.inf], np.nan)\n",
    "  inference_dataset= inference_dataset.fillna(inference_dataset.mean())\n",
    "  if features_col == '': \n",
    "    features = train.drop(columns=['Preis'],axis=1).columns\n",
    "    # features =['Nettogewicht','Länge in mm','Nirosta','Stärke','Biegungen','Durchmesser_innen','Fertigungsherkunft',\n",
    "    #       'Wettbewerbsintensität', 'Komplexität', 'Nutzungsdauer', 'Militärische Verwendung', 'Baureihe'\n",
    "    # ]\n",
    "  else:\n",
    "    features = features_col\n",
    "  target = 'Preis'\n",
    "\n",
    "  Y = train['Preis']\n",
    "  X = train.drop(columns=['Preis'])\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X[features], Y, test_size=0.15, random_state=9)\n",
    "\n",
    "\n",
    "  print('X train shape: ', X_train.shape)\n",
    "  print('Y train shape: ', Y_train.shape)\n",
    "  print('X test shape: ', X_test.shape)\n",
    "  print('Y test shape: ', Y_test.shape)\n",
    "  # X[features].head()\n",
    "  print(features)\n",
    "\n",
    "\n",
    "  # We define the model\n",
    "  estimator = RandomForestRegressor(random_state = 42,criterion='squared_error')\n",
    "  para_grids = {\n",
    "              \"n_estimators\" : [10,50,100,200],\n",
    "              #\"n_estimators\" : [10,50,100],\n",
    "              \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n",
    "              'max_depth' : [1,2,3,4,5,6,7,8,9,10],\n",
    "              #'max_depth' : [3,5,7,9,11,13],\n",
    "              \"bootstrap\"    : [True, False]\n",
    "          }\n",
    "  # para_grids = {\n",
    "  #           \"n_estimators\" : [10,50],\n",
    "  #           #\"n_estimators\" : [10,50],\n",
    "  #           # \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n",
    "  #           'max_depth' : [3,4],\n",
    "  #           # 'max_depth' : [3,5,7,9,11,13],\n",
    "  #           # \"bootstrap\"    : [True, False]\n",
    "  #       }\n",
    "\n",
    "  inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "  outer_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "  # Non_nested parameter search and scoring\n",
    "  clf = GridSearchCV(estimator=estimator, param_grid=para_grids, cv=outer_cv)\n",
    "  clf.fit(train[features], train[target])\n",
    "  non_nested_score = clf.best_score_\n",
    "  print('Non Nested Score: %.3f' % (non_nested_score))\n",
    "  best_param = clf.best_estimator_\n",
    "  print(best_param)\n",
    "\n",
    "  # Nested CV with parameter optimization\n",
    "  clf = GridSearchCV(estimator=estimator, param_grid=para_grids, cv=inner_cv)\n",
    "  n_score = cross_val_score(clf, X=train[features], y=train[target], cv=outer_cv)\n",
    "  nested_score = n_score.mean()\n",
    "  # The following is the most realistic performance estimate for the model selection pipeline!\n",
    "  print('Nested Score: %.3f' % (nested_score))\n",
    "  score_difference = non_nested_score - nested_score\n",
    "  print('Score Difference: %.3f' % (score_difference))\n",
    "\n",
    "  # Nested CV with parameter optimization to yield predictions on the entire training set\n",
    "  clf = GridSearchCV(estimator=estimator, param_grid=para_grids, cv=inner_cv)\n",
    "  predictions = cross_val_predict(clf, X=train[features], y=train[target], cv=outer_cv)\n",
    "  train['Preis_Predicted']= predictions\n",
    "\n",
    "  train.to_excel(\"output.xlsx\", sheet_name='Output_Hydraulikleitungen') \n",
    "\n",
    "  mean = \"Mean Squared Error: \" + str(sqrt(mean_squared_error(train.Preis, train.Preis_Predicted))) + \" \\n\"  \n",
    "  msr   =        \"Mean Squared Error:  \" + str(sqrt(mean_squared_error(train.Preis, train.Preis_Predicted)))  + \" \\n\"\n",
    "  mar    =    \"Mean Absolute Error:  \"+ str(mean_absolute_error(train.Preis, train.Preis_Predicted)) + \" \\n\"\n",
    "  mar2    = \"Median Absolute Error:  \"+ str(median_absolute_error(train.Preis, train.Preis_Predicted)) + \" \\n\"\n",
    "  r2   = \"R2 Score: \"+ str(r2_score(train.Preis, train.Preis_Predicted))\n",
    "  result = mean + msr + mar + mar2 + r2  \n",
    "  params = {\"n_estimators\":best_param.n_estimators ,\n",
    "            \"max_features\":best_param.max_features,\n",
    "            \"bootstrap\":best_param.bootstrap,\n",
    "            \"max_depth\":best_param.max_depth}\n",
    "  print('Mean Squared Error: %.3f' % (mean_squared_error(train.Preis, train.Preis_Predicted)))\n",
    "\n",
    "  print('Mean Squared Error: %.3f' % sqrt(mean_squared_error(train.Preis, train.Preis_Predicted)))\n",
    "  print('Mean Absolute Error: %.3f' % (mean_absolute_error(train.Preis, train.Preis_Predicted)))\n",
    "  print('Median Absolute Error: %.3f' % (median_absolute_error(train.Preis, train.Preis_Predicted)))\n",
    "  print('R2 Score: %.3f' % (r2_score(train.Preis, train.Preis_Predicted)))\n",
    "\n",
    "  inference_predictions = best_param.predict(inference_dataset[features])\n",
    "\n",
    "\n",
    "  features =['Länge in mm','Nirosta','Stärke','Biegungen','Durchmesser_innen','Fertigungsherkunft',\n",
    "        'Wettbewerbsintensität', 'Komplexität', 'Nutzungsdauer', 'Militärische Verwendung', 'Baureihe']\n",
    "  submission = pd.DataFrame({'ID':inference_dataset['ID'],'Länge in mm':inference_dataset['Länge in mm'],'Nettogewicht':inference_dataset['Nettogewicht'],'Biegungen':inference_dataset['Biegungen'],'Durchmesser_innen':inference_dataset['Durchmesser_innen'],'Komplexität':inference_dataset['Komplexität'],'Militärische Verwendung':inference_dataset['Militärische Verwendung'],'Preis':inference_predictions})                        \n",
    "\n",
    "\n",
    "  filename = 'submission_hydraulic_pipe1.csv'\n",
    "\n",
    "  submission.to_csv(filename,index=True)\n",
    "\n",
    "\n",
    "  print('Saved file: ' + filename)\n",
    "  return result,params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8oik_X4-IRHL"
   },
   "outputs": [],
   "source": [
    "def train_withparams(features_col='',n_estimators=100,max_depth=4,max_features =\"auto\",bootstrap=True, random_state=42):\n",
    "  train = pd.read_excel(\"/content/train data.xlsx\")\n",
    "  inference_dataset = pd.read_excel(\"/content/test data.xlsx\")\n",
    " \n",
    "  ep = train['Länge in mm'].value_counts().reset_index()\n",
    "  ep.columns = [\n",
    "      'Länge', \n",
    "      'percent'\n",
    "  ]\n",
    "  ep['percent'] /= len(train)\n",
    "\n",
    "\n",
    "  ep = train['Biegungen'].value_counts().reset_index()\n",
    "  ep.columns = [\n",
    "      'Biegungen', \n",
    "      'percent'\n",
    "  ]\n",
    "  ep['percent'] /= len(train)\n",
    "\n",
    "  labels = {}\n",
    "  for col in train.select_dtypes(exclude = np.number).columns.tolist():\n",
    "      le = LabelEncoder().fit(pd.concat([train[col].astype(str),inference_dataset[col].astype(str)]))   \n",
    "      train[col] = le.transform(train[col].astype(str))\n",
    "      inference_dataset[col] = le.transform(inference_dataset[col].astype(str))\n",
    "      labels [col] = le\n",
    "  print('Categorical columns:', list(labels.keys()))\n",
    "\n",
    "\n",
    "\n",
    "  print(f'Percent of Nans in Train Data : {round(train.isna().sum().sum()/len(train), 2)}')\n",
    "  print(f'Percent of Nans in Test  Data : {round(inference_dataset.isna().sum().sum()/len(inference_dataset), 2)}')\n",
    "\n",
    "  train = train.replace([np.inf, -np.inf], np.nan)\n",
    "  train= train.fillna(train.mean())\n",
    "\n",
    "\n",
    "  inference_dataset = inference_dataset.replace([np.inf, -np.inf], np.nan)\n",
    "  inference_dataset= inference_dataset.fillna(inference_dataset.mean())\n",
    "\n",
    "  if features_col == '': \n",
    "    features = train.drop(columns=['Preis'],axis=1).columns\n",
    "    # features =['Nettogewicht','Länge in mm','Nirosta','Stärke','Biegungen','Durchmesser_innen','Fertigungsherkunft',\n",
    "    #       'Wettbewerbsintensität', 'Komplexität', 'Nutzungsdauer', 'Militärische Verwendung', 'Baureihe'\n",
    "    # ]\n",
    "  else:\n",
    "   features = features_col\n",
    "  target = 'Preis'\n",
    "\n",
    "  Y = train['Preis']\n",
    "  X = train.drop(columns=['Preis'])\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X[features], Y, test_size=0.15, random_state=random_state)\n",
    "\n",
    "  print('X train shape: ', X_train.shape)\n",
    "  print('Y train shape: ', Y_train.shape)\n",
    "  print('X test shape: ', X_test.shape)\n",
    "  print('Y test shape: ', Y_test.shape)\n",
    "  # X[features].head()\n",
    "  print(features)\n",
    "\n",
    "\n",
    "  # We define the model\n",
    "  estimator = RandomForestRegressor( max_depth=max_depth,max_features=max_features  , n_estimators=n_estimators,bootstrap=bootstrap, random_state = 42,criterion='squared_error')\n",
    "  estimator.fit(train[features], train[target])\n",
    "\n",
    "  \n",
    "  predictions = estimator.predict(train[features])\n",
    "  train['Preis_Predicted']= predictions\n",
    "\n",
    "  train.to_excel(\"output.xlsx\", sheet_name='Output_Hydraulikleitungen') \n",
    "\n",
    "  mean = \"Mean Squared Error: \" + str(sqrt(mean_squared_error(train.Preis, train.Preis_Predicted))) + \" \\n\"  \n",
    "  msr   =        \"Mean Squared Error:  \" + str(sqrt(mean_squared_error(train.Preis, train.Preis_Predicted)))  + \" \\n\"\n",
    "  mar    =    \"Mean Absolute Error:  \"+ str(mean_absolute_error(train.Preis, train.Preis_Predicted)) + \" \\n\"\n",
    "  mar2    = \"Median Absolute Error:  \"+ str(median_absolute_error(train.Preis, train.Preis_Predicted)) + \" \\n\"\n",
    "  r2   = \"R2 Score: \"+ str(r2_score(train.Preis, train.Preis_Predicted))\n",
    "  result = mean + msr + mar + mar2 + r2  \n",
    "  \n",
    "\n",
    "  inference_predictions = estimator.predict(inference_dataset[features])\n",
    "\n",
    "  features =['Länge in mm','Nirosta','Stärke','Biegungen','Durchmesser_innen','Fertigungsherkunft',\n",
    "        'Wettbewerbsintensität', 'Komplexität', 'Nutzungsdauer', 'Militärische Verwendung', 'Baureihe']\n",
    "  submission = pd.DataFrame({'ID':inference_dataset['ID'],'Länge in mm':inference_dataset['Länge in mm'],'Nettogewicht':inference_dataset['Nettogewicht'],'Biegungen':inference_dataset['Biegungen'],'Durchmesser_innen':inference_dataset['Durchmesser_innen'],'Komplexität':inference_dataset['Komplexität'],'Militärische Verwendung':inference_dataset['Militärische Verwendung'],'Preis':inference_predictions})                        \n",
    "\n",
    "\n",
    "  filename = 'submission_hydraulic_pipe1.csv'\n",
    "\n",
    "  submission.to_csv(filename,index=True)\n",
    "\n",
    "\n",
    "  print('Saved file: ' + filename)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7EAyURXX5b2W"
   },
   "outputs": [],
   "source": [
    "# train_withparams()\n",
    "# pd.read_excel('train data.xlsx').drop(columns=['ID'],axis=1,inplace=True) \n",
    "# pd.read_excel(\"/content/train data.xlsx\")['Wert Nirosta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "id": "m5KyhYWEdGNm",
    "outputId": "2fa8f014-d76f-432a-cdcd-f23d4b7df4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['Fertigungsherkunft', 'Baureihe Datum']\n",
      "Percent of Nans in Train Data : 0.1\n",
      "Percent of Nans in Test  Data : 1.38\n",
      "X train shape:  (211, 24)\n",
      "Y train shape:  (211,)\n",
      "X test shape:  (38, 24)\n",
      "Y test shape:  (38,)\n",
      "Index(['Nettogewicht', 'Länge in mm', 'Länge', 'Nirosta', 'Wert Nirosta',\n",
      "       'Stärke', 'Biegungen', 'Wert Biegungen', 'Durchmesser_aussen',\n",
      "       'Wert Durchmesser', 'Durchmesser_innen', 'Fertigungsherkunft',\n",
      "       'Wert Fertigungsherkunft', 'Wettbewerbsintensität',\n",
      "       'Wert Wettbewerbsintensität', 'Komplexität', 'Wert Komplexität',\n",
      "       'Nutzungsdauer', 'Militärische Verwendung',\n",
      "       'Wert Militärische Verwendung', 'Baureihe Datum', 'Baureihe',\n",
      "       'Wert Baureihe', 'Value'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-2173546dfd47>:40: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  inference_dataset= inference_dataset.fillna(inference_dataset.mean())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Nested Score: 0.949\n",
      "RandomForestRegressor(max_depth=4, n_estimators=50, random_state=42)\n",
      "Nested Score: 0.946\n",
      "Score Difference: 0.003\n",
      "Mean Squared Error: 162.059\n",
      "Mean Squared Error: 12.730\n",
      "Mean Absolute Error: 8.571\n",
      "Median Absolute Error: 6.159\n",
      "R2 Score: 0.947\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9a42d12a3416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0manvil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlobMedia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"application/vnd.ms-excel\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0manvil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/anvil/server.py\u001b[0m in \u001b[0;36mwait_forever\u001b[0;34m()\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "anvil.server.connect(\"server_FPS4H4JV6LL2OEIZMQY4M66E-7VNAX7SHYSCBEHWO\")\n",
    "\n",
    "@anvil.server.callable\n",
    "def say_hello(name):\n",
    "  print(\"Hello from the uplink, %s!\" % name)\n",
    "# anvil.server.wait_forever()\n",
    "\n",
    "\n",
    "@anvil.server.callable\n",
    "def calculate_model_withparam(features, n_estimators,max_depth,max_features ,bootstrap):\n",
    "  res = train_withparams(features,n_estimators,max_depth,max_features ,bootstrap)\n",
    "  df1 = pd.read_csv(\"/content/submission_hydraulic_pipe1.csv\")\n",
    "  content = io.BytesIO()\n",
    "  df1.to_csv(content, index=False)\n",
    "  content.seek(0, 0)\n",
    "  return anvil.BlobMedia(content=content.read(), content_type=\"text/csv\",name='output.csv'), res\n",
    "  # csv_media = anvil.BlobMedia(\"text/plain\", df1, name=\"Report.csv\")\n",
    "  # return \n",
    "@anvil.server.callable\n",
    "def calculate_model(features_col):\n",
    "  res,params = train_eval(features_col)\n",
    "  df1 = pd.read_csv(\"/content/submission_hydraulic_pipe1.csv\")\n",
    "  content = io.BytesIO()\n",
    "  df1.to_csv(content, index=False)\n",
    "  content.seek(0, 0)\n",
    "  return anvil.BlobMedia(content=content.read(), content_type=\"text/csv\",name='output.csv'), res,params\n",
    "  # csv_media = anvil.BlobMedia(\"text/plain\", df1, name=\"Report.csv\")\n",
    "  # return \n",
    "i = 0\n",
    "@anvil.server.callable\n",
    "def check_files():\n",
    "  df1 = \"\"\n",
    "  try:\n",
    "    df1 = pd.read_excel(\"/content/train data.xlsx\")\n",
    "  except:\n",
    "    return \"no\"\n",
    "  content = io.BytesIO()\n",
    "  df1.to_csv(content, index=False)\n",
    "  content.seek(0, 0)\n",
    "  return anvil.BlobMedia(content=content.read(), content_type=\"text/csv\",name='output.csv')\n",
    "\n",
    "@anvil.server.callable\n",
    "def upload_excel_data(file,name):\n",
    "  # for i,file in enumerate(files):\n",
    "  with open(name, \"wb\") as f:\n",
    "    f.write(file.get_bytes())\n",
    "    # df = pd.read_csv(f)\n",
    "  return  str(i)\n",
    "def download_out():\n",
    "  df1 = pd.read_excel(\"/content/ss.xlsx\")\n",
    "  content = io.BytesIO()\n",
    "  df1.to_excel(content, index=False)\n",
    "  content.seek(0, 0)\n",
    "  return anvil.BlobMedia(content=content.read(), content_type=\"application/vnd.ms-excel\",name='output.xlsx')\n",
    "anvil.server.wait_forever()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
